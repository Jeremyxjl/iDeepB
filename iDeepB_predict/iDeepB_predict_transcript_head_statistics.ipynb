{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/data/xliu/work/20231211_iDeepB/20240401_trainning_gene/iDeepB_train_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import scipy\n",
    "from Bio.Seq import reverse_complement,transcribe,Seq\n",
    "import torch\n",
    "import pandas as pd\n",
    "#from pybedtools import Interval\n",
    "import numpy as np\n",
    "import pysam\n",
    "import pyBigWig\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "sys.path.append('/data/xliu/work/') \n",
    "from iDeepB.iDeepB.utils.functions import onehot_encode\n",
    "\n",
    "from iDeepB.iDeepB.preprocessing.RBP_util import Track\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    def __init__(self, bigWigPlus, bigWigMinus):\n",
    "        self._bigWigPlus = pyBigWig.open(bigWigPlus)\n",
    "        self._bigWigMinus = pyBigWig.open(bigWigMinus)\n",
    "    def profile(self, chrInfo, startPos, endPos, strand):\n",
    "        if strand == '+':\n",
    "            signalList = self._bigWigPlus.values(chrInfo, int(startPos), int(endPos), numpy=False)\n",
    "            profile = np.nan_to_num(signalList, nan=0.0).tolist()\n",
    "\n",
    "        elif strand == '-':\n",
    "            signalList = self._bigWigMinus.values(chrInfo, int(startPos), int(endPos), numpy=False)\n",
    "            signalList = np.nan_to_num(signalList, nan=0.0)\n",
    "            profile = list(reversed(signalList))\n",
    "        else:\n",
    "            raise ValueError(f'Unspected strand: {strand}')\n",
    "        return profile\n",
    "\n",
    "class genomeSeqClasss:\n",
    "    def __init__(self, genomeFaFile):\n",
    "        # import genome fa file\n",
    "        self._genomeFa = pysam.FastaFile(genomeFaFile) #args.genomeFa\n",
    "    def seqFetch(self, chrInfo, startPos, endPos, strand):\n",
    "        if strand == '+':\n",
    "            seq = self._genomeFa.fetch(chrInfo, startPos, endPos)\n",
    "        elif strand == '-':\n",
    "            seq = self._genomeFa.fetch(chrInfo, startPos, endPos)\n",
    "            seq = reverse_complement(seq)\n",
    "        else:\n",
    "            raise ValueError(f'Unspected strand: {strand}')\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def predictFun(seqTranscribed, window_size, signalList):\n",
    "def predictHead(seqTranscribed, window_size, signalList, model, mode, codeModel = \"OneHot\"):\n",
    "    ## padding and extract subSeq or subSignal\n",
    "    remainder = (len(seqTranscribed) % window_size)\n",
    "\n",
    "    print(f\"## remainder Length is: {remainder}\")\n",
    "\n",
    "    if mode == \"Fragment\":\n",
    "        # 计算列表可以分成多少个子列表\n",
    "        num_sublists = len(seqTranscribed) // window_size\n",
    "\n",
    "        # 使用列表推导式创建子列表\n",
    "        sublists = [seqTranscribed[i * window_size: (i + 1) * window_size] for i in range(num_sublists)]\n",
    "\n",
    "        # 如果剩余元素不为零，将其加入最后一个子列表\n",
    "        remainder = len(seqTranscribed) % window_size\n",
    "        if remainder:\n",
    "            sublists.append(seqTranscribed[-window_size:])\n",
    "            \n",
    "    elif mode == \"midBase\":\n",
    "        sublists = [seqTranscribed[i: i + window_size] for i in np.arange(0, len(seqTranscribed)-window_size + 1, step=1)]\n",
    "\n",
    "        # for reminder 直接取第一个和最后一个就好\n",
    "        # sublists_end = [seqTranscribed[:window_size], seqTranscribed[-window_size:]]\n",
    "        # sublists = sublists + sublists_end\n",
    "    # print(sublists)\n",
    "    if codeModel == \"OneHot\":\n",
    "        vocab = list(\"AUGC\")\n",
    "        seqsInt = onehot_encode(sublists, vocab, 4)\n",
    "        subseqOH = torch.from_numpy(np.asarray(seqsInt) ).to(args.device)\n",
    "    elif codeModel == \"Embedding\":\n",
    "        seqsInt = [sequence2int(seq) for seq in sublists]\n",
    "    subseqOH = torch.from_numpy(np.asarray(seqsInt) ).to(args.device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        temp = []\n",
    "\n",
    "        if(EncodeMode == \"Embedding\"):\n",
    "            subseqOH = subseqOH.int()\n",
    "        elif(EncodeMode == \"OneHot\"):\n",
    "            subseqOH = subseqOH.float()\n",
    "        #elif(args.modelFramework == \"Enformer\"):\n",
    "            #subseqPredicted = model(subseqOH) \n",
    "        if mode == \"Fragment\":\n",
    "        \n",
    "            if remainder:\n",
    "                for subseqOHTurn in subseqOH[:-1].split(500,  0): #预测转录本，数据太大，无法预\n",
    "                    treat_predict = model(subseqOHTurn) #torch.Size([n, 101])\n",
    "                    treat_predict = treat_predict.detach().cpu().numpy().tolist()\n",
    "                    temp.extend(treat_predict)\n",
    "                temp = np.array(temp).reshape(-1).tolist()\n",
    "                \n",
    "                print(\"first temp len\", len(temp))\n",
    "                # for remainder length\n",
    "                treat_predict_end = model(subseqOHTurn[-1:]) #torch.Size([n, 101])\n",
    "                treat_predict_end = treat_predict_end.detach().cpu().numpy().tolist()\n",
    "\n",
    "                print(remainder, len(temp), len(np.array(treat_predict_end).reshape(-1).tolist()[-remainder:]), len(np.array(treat_predict_end).reshape(-1).tolist()))\n",
    "                temp = temp + np.array(treat_predict_end).reshape(-1).tolist()[-remainder:]\n",
    "                print(\"remainder: \",remainder, len(treat_predict_end[-remainder:]), len(temp))\n",
    "            else:\n",
    "                for subseqOHTurn in subseqOH.split(500,  0): #预测转录本，数据太大，无法预\n",
    "                    treat_predict = model(subseqOHTurn) #torch.Size([n, 101])\n",
    "\n",
    "                    treat_predict = treat_predict.detach().cpu().numpy().tolist()\n",
    "\n",
    "                    temp.extend(treat_predict)\n",
    "                temp = np.array(temp).reshape(-1).tolist()    \n",
    "        elif mode == \"midBase\":  #column_values = tensor_2d[:, 1]\n",
    "            for subseqOHTurn in subseqOH.split(1000,  0): #预测转录本，数据太大，无法预 [0:-0] 非[1:-1]\n",
    "                subseqPredicted = model(subseqOHTurn) #torch.Size([n, 101])\n",
    "                treat_predict = subseqPredicted\n",
    "   \n",
    "                treat_predict = treat_predict[:, window_size//2]\n",
    "                treat_predict = treat_predict.detach().cpu().numpy().tolist()\n",
    "\n",
    "                temp.extend(treat_predict)\n",
    "\n",
    "            # for remainder length\n",
    "            # 对于前半段 remainder_index == 0\n",
    "            remainder_index = 0\n",
    "            outputs_end = model(subseqOH[remainder_index].unsqueeze(0)) #torch.Size([n, 101])\n",
    "            w_end = outputs_end[2]\n",
    "            w_end = w_end.unsqueeze(axis=1)\n",
    "            \n",
    "            treat_predict_end = (outputs_end[0]*w_end) + (1-w_end)*outputs_end[1]\n",
    "            treat_predict_end = treat_predict_end.detach().cpu().numpy()\n",
    "            \n",
    "            temp = treat_predict_end[:, :window_size//2].reshape(-1).tolist() + temp\n",
    "\n",
    "            # 对于后半段\n",
    "            remainder_index = -1\n",
    "            treat_predict_end = model(subseqOH[remainder_index].unsqueeze(0)) #torch.Size([n, 101])\n",
    "\n",
    "            treat_predict_end = treat_predict_end.detach().cpu().numpy()\n",
    "\n",
    "            temp = treat_predict_end[:, :window_size//2].reshape(-1).tolist() + temp\n",
    "                \n",
    "\n",
    "        transcriptPd = np.array(temp) #subseqPredicted.cpu().detach().numpy()\n",
    "        # cor 1: subseq预测结果合并成一个向量，然后与实际信号计算相关性\n",
    "        # transcriptPd = subseqPredictedNp.reshape(-1)\n",
    "        print(\"Signal lenth of prediction and transcript length !\", len(transcriptPd), len(signalList))\n",
    "        if(len(transcriptPd) != len(signalList)):\n",
    "            print(\"Error: signal lenth not equal transcript length !\", len(transcriptPd), len(signalList))\n",
    "            return False\n",
    "    return transcriptPd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict2HeadW(seqTranscribed, window_size, signalList, model, mode, codeModel = \"OneHot\"):  #mode \"Fragment\" midBase\n",
    "    ## padding and extract subSeq or subSignal\n",
    "    remainder = (len(seqTranscribed) % window_size)\n",
    "\n",
    "    print(f\"## remainder Length is: {remainder}\")\n",
    "\n",
    "    if mode == \"Fragment\":\n",
    "        # 计算列表可以分成多少个子列表\n",
    "        num_sublists = len(seqTranscribed) // window_size\n",
    "\n",
    "        # 使用列表推导式创建子列表\n",
    "        sublists = [seqTranscribed[i * window_size: (i + 1) * window_size] for i in range(num_sublists)]\n",
    "\n",
    "        # 如果剩余元素不为零，将其加入最后一个子列表\n",
    "        remainder = len(seqTranscribed) % window_size\n",
    "        if remainder:\n",
    "            sublists.append(seqTranscribed[-window_size:])\n",
    "            \n",
    "    elif mode == \"midBase\":\n",
    "        sublists = [seqTranscribed[i: i + window_size] for i in np.arange(0, len(seqTranscribed)-window_size + 1, step=1)]\n",
    "\n",
    "        # for reminder 直接取第一个和最后一个就好\n",
    "        # sublists_end = [seqTranscribed[:window_size], seqTranscribed[-window_size:]]\n",
    "        # sublists = sublists + sublists_end\n",
    "    # print(sublists)\n",
    "    if codeModel == \"OneHot\":\n",
    "        vocab = list(\"AUGC\")\n",
    "        seqsInt = onehot_encode(sublists, vocab, 4)\n",
    "        subseqOH = torch.from_numpy(np.asarray(seqsInt) ).to(args.device)\n",
    "    elif codeModel == \"Embedding\":\n",
    "        seqsInt = [sequence2int(seq) for seq in sublists]\n",
    "    subseqOH = torch.from_numpy(np.asarray(seqsInt) ).to(args.device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        temp = []\n",
    "\n",
    "        if(EncodeMode == \"Embedding\"):\n",
    "            subseqOH = subseqOH.int()\n",
    "        elif(EncodeMode == \"OneHot\"):\n",
    "            subseqOH = subseqOH.float()\n",
    "        #elif(args.modelFramework == \"Enformer\"):\n",
    "            #subseqPredicted = model(subseqOH) \n",
    "        if mode == \"Fragment\":\n",
    "        \n",
    "            if remainder:\n",
    "                for subseqOHTurn in subseqOH[:-1].split(500,  0): #预测转录本，数据太大，无法预\n",
    "                    outputs = model(subseqOHTurn) #torch.Size([n, 101])\n",
    "                    w = outputs[2]\n",
    "                    w = w.unsqueeze(axis=1)\n",
    "                    \n",
    "                    treat_predict = (outputs[0]*w) + (1-w)*outputs[1]\n",
    "                    treat_predict = treat_predict.detach().cpu().numpy().tolist()\n",
    "\n",
    "                    temp.extend(treat_predict)\n",
    "                temp = np.array(temp).reshape(-1).tolist()\n",
    "                \n",
    "                print(\"first temp len\", len(temp))\n",
    "                # for remainder length\n",
    "                outputs_end = model(subseqOHTurn[-1:]) #torch.Size([n, 101])\n",
    "                w_end = outputs_end[2]\n",
    "                w_end = w_end.unsqueeze(axis=1)\n",
    "                \n",
    "                treat_predict_end = (outputs_end[0]*w_end) + (1-w_end)*outputs_end[1]\n",
    "                treat_predict_end = treat_predict_end.detach().cpu().numpy().tolist()\n",
    "\n",
    "                print(remainder, len(temp), len(np.array(treat_predict_end).reshape(-1).tolist()[-remainder:]), len(np.array(treat_predict_end).reshape(-1).tolist()))\n",
    "                temp = temp + np.array(treat_predict_end).reshape(-1).tolist()[-remainder:]\n",
    "                print(\"remainder: \",remainder, len(treat_predict_end[-remainder:]), len(temp))\n",
    "            else:\n",
    "                for subseqOHTurn in subseqOH.split(500,  0): #预测转录本，数据太大，无法预\n",
    "                    outputs = model(subseqOHTurn) #torch.Size([n, 101])\n",
    "                    w = outputs[2]\n",
    "                    w = w.unsqueeze(axis=1)\n",
    "                    \n",
    "                    treat_predict = (outputs[0]*w) + (1-w)*outputs[1]\n",
    "                    treat_predict = treat_predict.detach().cpu().numpy().tolist()\n",
    "\n",
    "                    temp.extend(treat_predict)\n",
    "                temp = np.array(temp).reshape(-1).tolist()    \n",
    "        elif mode == \"midBase\":  #column_values = tensor_2d[:, 1]\n",
    "            for subseqOHTurn in subseqOH.split(1000,  0): #预测转录本，数据太大，无法预 [0:-0] 非[1:-1]\n",
    "                subseqPredicted = model(subseqOHTurn) #torch.Size([n, 101])\n",
    "                outputs = subseqPredicted\n",
    "                w = outputs[2]\n",
    "                w = w.unsqueeze(axis=1)\n",
    "                \n",
    "                treat_predict = (outputs[0]*w) + (1-w)*outputs[1]\n",
    "                treat_predict = treat_predict[:, window_size//2]\n",
    "                treat_predict = treat_predict.detach().cpu().numpy().tolist()\n",
    "\n",
    "                temp.extend(treat_predict)\n",
    "\n",
    "            # for remainder length\n",
    "            # 对于前半段 remainder_index == 0\n",
    "            remainder_index = 0\n",
    "            outputs_end = model(subseqOH[remainder_index].unsqueeze(0)) #torch.Size([n, 101])\n",
    "            w_end = outputs_end[2]\n",
    "            w_end = w_end.unsqueeze(axis=1)\n",
    "            \n",
    "            treat_predict_end = (outputs_end[0]*w_end) + (1-w_end)*outputs_end[1]\n",
    "            treat_predict_end = treat_predict_end.detach().cpu().numpy()\n",
    "            \n",
    "            temp = treat_predict_end[:, :window_size//2].reshape(-1).tolist() + temp\n",
    "\n",
    "            # 对于后半段\n",
    "            remainder_index = -1\n",
    "            outputs_end = model(subseqOH[remainder_index].unsqueeze(0)) #torch.Size([n, 101])\n",
    "            w_end = outputs_end[2]\n",
    "            w_end = w_end.unsqueeze(axis=1)\n",
    "            \n",
    "            treat_predict_end = (outputs_end[0]*w_end) + (1-w_end)*outputs_end[1]\n",
    "            treat_predict_end = treat_predict_end.detach().cpu().numpy()\n",
    "\n",
    "            temp = treat_predict_end[:, :window_size//2].reshape(-1).tolist() + temp\n",
    "                \n",
    "\n",
    "        transcriptPd = np.array(temp) #subseqPredicted.cpu().detach().numpy()\n",
    "        # cor 1: subseq预测结果合并成一个向量，然后与实际信号计算相关性\n",
    "        # transcriptPd = subseqPredictedNp.reshape(-1)\n",
    "        print(\"Signal lenth of prediction and transcript length !\", len(transcriptPd), len(signalList))\n",
    "        if(len(transcriptPd) != len(signalList)):\n",
    "            print(\"Error: signal lenth not equal transcript length !\", len(transcriptPd), len(signalList))\n",
    "            return False\n",
    "    return transcriptPd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_if_not_exists(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"文件夹 '{folder_path}' 已创建。\")\n",
    "    else:\n",
    "        print(f\"文件夹 '{folder_path}' 已经存在。\")\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "\n",
    "# 参数设置\n",
    "fasta_genome = \"/data/xliu/dataBase/GRCh38.p13.genome.fa\"\n",
    "\n",
    "#Encode_eCLIP_dataSet路径\n",
    "eCLIP_dataSet_path = \"/data/xliu/work/20231211_iDeepB/20240216_trainData/bam2bigwig/Encode_eCLIP_dataSet\"\n",
    "\n",
    "\n",
    "log = \"\"\n",
    "loss = \"poisson_loss_test\" # \"poisson_loss_test\"  #  kl_divergence_loss  multinomialnll_batch pearson mse poisson_loss_test\n",
    "model_dir = \"ideepB_trained_model\"\n",
    "args.mode = \"Fragment\"\n",
    "args.modelFramework =  \"ResLSTMMSA\" #\"CNNLSTM16\"\n",
    "args.outputFolder = \"iDeepB_predict_TP_head_ResLSTMMSA\"\n",
    "\n",
    "# 使用tab20c颜色映射\n",
    "point_colors = plt.cm.tab20c  #tab20c\n",
    "\n",
    "# create_folder_if_not_exists(f\"{output_dir}\")\n",
    "\n",
    "args.genomeFa = \"/home/xliu/dataBase/GRCh38.p13.genome.fa\"\n",
    "## genome fasta\n",
    "genomeFa = pysam.FastaFile(args.genomeFa) #\"/home/xliu/dataBase/GRCh38.p13.genome.fa\"\n",
    "genomeFa2 = genomeSeqClasss(args.genomeFa) \n",
    "    \n",
    "\n",
    "# eCLIP_dataSet_array = [folder.split(\"/\")[-1].split(\"_\") for folder in glob.glob(f'{eCLIP_dataSet_path}/*') ]\n",
    "\n",
    "BamInfo_file = \"ENCODE_eCLIP_252_BamInfo.txt\"\n",
    "\n",
    "\n",
    "# bam info input\n",
    "ENCODE_eCLIP_bamInfo = pd.read_table(BamInfo_file, header=0, sep = \"\\t\")\n",
    "ENCODE_eCLIP_bamInfo['term_name'] = ENCODE_eCLIP_bamInfo['term_name'].replace('adrenal gland', 'adrenalgland', regex=True)\n",
    "eCLIP_dataSet_DF = ENCODE_eCLIP_bamInfo # ENCODE_eCLIP_bamInfo[[\"symbol\", \"term_name\", \"accession\", \"run_type\"]]\n",
    "eCLIP_dataSet_DF.insert(loc=3, column='dataSet', value = \"dataSet\")\n",
    "eCLIP_dataSet_DF.insert(loc=4, column='correct', value = \"c0\")\n",
    "\n",
    "# main process \n",
    "corPearson = []\n",
    "corSpearman = []\n",
    "roc_auc = []\n",
    "roc_auc_prc = []\n",
    "peakCountList = []\n",
    "TP_name = []\n",
    "RBP_name = []\n",
    "\n",
    "\n",
    "for index ,resies_row in eCLIP_dataSet_DF[34:35].iterrows():\n",
    "    print(f\"Doing: {index}\")\n",
    "\n",
    "    if (resies_row.term_name == \"adrenal gland\"):\n",
    "        resies_row.term_name = \"adrenalgland\"\n",
    "\n",
    "    protein = resies_row.symbol\n",
    "    cell = resies_row.term_name\n",
    "    task = f'{protein}_{cell}_{resies_row.accession}'\n",
    "    runtype = resies_row.run_type\n",
    "    correct = resies_row.correct\n",
    "    eclipID = resies_row.accession\n",
    "\n",
    "\n",
    "    if log == \"\":\n",
    "        args.log = False\n",
    "    if log != \"\":\n",
    "        args.log = True\n",
    "\n",
    "    args.sample = f\"{protein}_{cell}\"\n",
    "    args.transcriptBed = f\"/data/xliu/work/20231211_iDeepB/20240307_transcript_predict/TP_with_peak/TP_with_peak/{task}_TP_with_peak_rmDup.txt\"\n",
    "    args.window_size = 101\n",
    "    args.output = f\"{args.outputFolder}/{protein}_{cell}_train_101_encode_c0_{loss}_test_{log}_{args.modelFramework}\" # f\"QKI_{cell}_train_101_encode_c0_{lossFun}_{args.modelFramework}_Fragment_TP\"\n",
    "\n",
    "    args.modelPath =  f\"{model_dir}/{task}_c0/model_params.pth\" \n",
    "  \n",
    "    \n",
    "    args.plus_bw = f\"/data/xliu/work/20231211_iDeepB/20240216_trainData/bam2bigwig/Encode_eCLIP_dataSet/{task}_dataSet_{correct}_{runtype}/{task}.pos.bigWig\"\n",
    "    args.minus_bw = f\"/data/xliu/work/20231211_iDeepB/20240216_trainData/bam2bigwig/Encode_eCLIP_dataSet/{task}_dataSet_{correct}_{runtype}/{task}.neg.bigWig\"\n",
    "    \n",
    "    bwTrack = Track(args.plus_bw, args.minus_bw)\n",
    "    \n",
    "    args.CUDA = \"0\"\n",
    "    codeModel = \"OneHot\"\n",
    "    EncodeMode = \"OneHot\"\n",
    "    mode = args.mode\n",
    "    window_size = args.window_size\n",
    "    if(mode == \"midBase\"):\n",
    "        subStride = 1\n",
    "    elif(mode == \"Fragment\"):\n",
    "        subStride = window_size\n",
    "\n",
    "    args.slide_len = 0\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.CUDA)\n",
    "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    args.Signal_OP = False\n",
    "\n",
    "    transcriptBed =  pd.read_table(args.transcriptBed, sep=\"\\t\", header=None, names=[\"chrom_TP\", \"start_TP\", \"stop_TP\", \"name_TP\", \"score_TP\", \"strand_TP\", \"chr\", \"start\", \"end\", \"name\", \"strand\", 'signalValue', 'pValue',\"Count\"])\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "    if args.modelFramework == \"ResLSTMMSA\":\n",
    "        print(\"Model: \", args.modelFramework)\n",
    "        # from iDeepB.iDeepB.models.ResLSTMMSA import iDeepB  #ResLSTMMSA_v1\n",
    "        from iDeepB.iDeepB.models.ResLSTMMSA_v1 import iDeepB  #ResLSTMMSA_v1\n",
    "        model = iDeepB(dropOut=0.).to(args.device)\n",
    "    elif(args.modelFramework == \"ResLSTMMSA2headw\"): \n",
    "        print(\"Model: \", args.modelFramework)\n",
    "        from iDeepB.iDeepB.models.ResLSTMMSA2headw import iDeepB\n",
    "        model = iDeepB(dropOut = 0.).to(args.device)\n",
    "    elif(args.modelFramework == \"treat\"): \n",
    "        print(\"Model: \", args.modelFramework)\n",
    "        from iDeepB.iDeepB.models.models import iDeepB\n",
    "        model = iDeepB(dropOut = 0.).to(args.device)\n",
    "    elif(args.modelFramework == \"weight\"): \n",
    "        print(\"Model: \", args.modelFramework)\n",
    "        from iDeepB.iDeepB.models.models import iDeepB\n",
    "        model = iDeepB(dropOut = 0., control = True).to(args.device)\n",
    "    # print(\"Train model: \",model)\n",
    "    model.load_state_dict(torch.load(args.modelPath), strict=False)\n",
    "    print(mode)\n",
    "    ## inpput bw file\n",
    "    plus_bw = pyBigWig.open(args.plus_bw)#pyBigWig.open(\"/home/xliu/work/20230103_EPRB_model/TIA1_{cell}/TIA1_{cell}.pos.bigWig\") #\"ENCFF947JHV.secondR_128.5.pos.sorted.bigWig\"\n",
    "    minus_bw = pyBigWig.open(args.minus_bw)#pyBigWig.open(\"/home/xliu/work/20230103_EPRB_model/TIA1_{cell}/TIA1_{cell}.neg.bigWig\") #\"ENCFF947JHV.secondR_128.5.neg.sorted.bigWig\"\n",
    "\n",
    "    ## 需要预测的数据\n",
    "    #transcriptBed = args.transcriptBed#\"/home/xliu/work/20230103_EPRB_model/TIA1_{cell}/RBP.TIA1_{cell}.300.peak_test.transcript.fa.bed\"\n",
    "    # output file\n",
    "    outputDir = args.output\n",
    "    if(os.path.isdir(outputDir)):\n",
    "        shutil.rmtree(outputDir)\n",
    "        os.makedirs(outputDir)\n",
    "    else:\n",
    "        os.makedirs(outputDir)\n",
    "\n",
    "    if args.Signal_OP:\n",
    "        predictOPFile = f\"{outputDir}/RBP.predict.out\" # f\"{args.output}.RBP.predict.out\"\n",
    "        predictCorOPFile = f\"{outputDir}/RBP.predict.cor.out\" # f\"{args.output}.RBP.predict.cor.out\"\n",
    "        predictOP = open(predictOPFile, \"w+\")\n",
    "        predictCorOP = open(predictCorOPFile, \"w+\")\n",
    "\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    # 检测泛化能力\n",
    "    slide_len = args.slide_len\n",
    "    if(slide_len != 0):\n",
    "        corPearson_slide = []\n",
    "        corSpearman_slide = []\n",
    "\n",
    "    # 选择测试集部分\n",
    "    test_chr = [\"chr1\", \"chr8\", \"chr15\"]\n",
    "    print(f\"Before filter by length: {transcriptBed.shape}\")\n",
    "    transcriptBed = transcriptBed[transcriptBed[\"chrom_TP\"].isin(test_chr)]\n",
    "\n",
    "    transcriptBed_dup = transcriptBed.drop_duplicates(subset='name_TP')\n",
    "\n",
    "    transcriptBed_dup[\"length_TP\"] = transcriptBed_dup[\"stop_TP\"] - transcriptBed_dup[\"start_TP\"]\n",
    "\n",
    "    # 获取满足要求的转录本\n",
    "    # transcriptBed_dup = transcriptBed_dup[transcriptBed_dup[\"length_TP\"] < 50000]  \n",
    "    # 筛选的长度，变长，结果会下降很多\n",
    "    transcriptBed_dup = transcriptBed_dup[(transcriptBed_dup[\"length_TP\"] < 100000) & (transcriptBed_dup[\"length_TP\"] >= 101)]  \n",
    "    print(f\"After filter by length: {transcriptBed_dup.shape}\")\n",
    "\n",
    "    '''\n",
    "    # main process \n",
    "    corPearson = []\n",
    "    corSpearman = []\n",
    "    roc_auc = []\n",
    "    roc_auc_prc = []\n",
    "    peakCountList = []\n",
    "    TP_name = []\n",
    "    '''\n",
    "\n",
    "    TPList = list(transcriptBed_dup[\"name_TP\"].values)\n",
    "    for index_TP, transcriptName in tqdm(enumerate(TPList), total = len(TPList)):\n",
    "        TF_with_peak = transcriptBed[transcriptBed[\"name_TP\"] == transcriptName]\n",
    "\n",
    "        # 获得每个转录本，encode peak\n",
    "        TF_with_peak = TF_with_peak.reset_index()\n",
    "        for index_peak, transcript in TF_with_peak.iterrows():\n",
    "\n",
    "            if index_peak == 0:\n",
    "                label_list = np.array([0]* (transcript[\"stop_TP\"] - transcript[\"start_TP\"]))\n",
    "            label_start = transcript[\"start\"]- transcript[\"start_TP\"]\n",
    "            label_end = transcript[\"end\"]- transcript[\"start_TP\"]\n",
    "            label_list[label_start:label_end] = 1 \n",
    "\n",
    "        labelList = label_list\n",
    "\n",
    "        # 如果没有序列，给的基因组位置，直接从基因组获取序列\n",
    "        seq = genomeFa2.seqFetch(transcript.chrom_TP, int(transcript.start_TP), int(transcript.stop_TP), transcript.strand_TP)\n",
    "        seqTranscribed = transcribe(seq) #T->U\n",
    "        signalList = bwTrack.profile(transcript.chrom_TP, int(transcript.start_TP), int(transcript.stop_TP), transcript.strand_TP)\n",
    "\n",
    "        if \"N\" in seqTranscribed:\n",
    "            continue\n",
    "        RBP_name.append(task)\n",
    "        TP_name.append(transcriptName)\n",
    "\n",
    "        if(slide_len != 0):\n",
    "            seq_slide = genomeFa2.seqFetch(transcript.chrom_TP, int(transcript.start_TP)+slide_len, int(transcript.stop_TP)+slide_len, transcript.strand_TP)\n",
    "            seqTranscribed_slide = transcribe(seq_slide) #T->U\n",
    "            #signalList_slide = bwTrack.profile(transcript.chrom_TP, int(transcript.start_TP)+slide_len, int(transcript.stop_TP)+slide_len, transcript.strand_TP)\n",
    "\n",
    "        # 如果转录本上面没有结合信号，则不进行预测\n",
    "        if(sum(signalList) == 0):\n",
    "            corPearson.append(np.nan)\n",
    "            roc_auc.append(np.nan)\n",
    "            roc_auc_prc.append(np.nan)\n",
    "            peakCountList.append(0)\n",
    "            continue\n",
    "        print(f\"# TP index: {index} {transcriptName}: TP length = {len(signalList)}, signal sum = {sum(signalList)}, signal max = {max(signalList)}\")\n",
    "\n",
    "        if(mode == \"Fragment\"):\n",
    "            print(f\"mode: {mode}\")\n",
    "            # 预测\n",
    "            if (args.log):\n",
    "                transcriptPd = np.exp(predictHead(seqTranscribed, window_size, signalList, model, mode=\"Fragment\"))-1\n",
    "            else:\n",
    "                transcriptPd = predictHead(seqTranscribed, window_size, signalList, model, mode=\"Fragment\")\n",
    "\n",
    "            # transcriptPd = transcriptPd[0]\n",
    "\n",
    "            # 通过平移sequence，测试模型泛化能力\n",
    "            if(slide_len>0):\n",
    "                if (args.log):\n",
    "                    transcriptPd_slide = np.exp(predictHead(seqTranscribed_slide, window_size, signalList, model, mode=\"Fragment\"))-1\n",
    "                else:\n",
    "                    transcriptPd_slide = predictHead(seqTranscribed_slide, window_size, signalList, model, mode=\"Fragment\")\n",
    "\n",
    "                transcriptPdpearsonCor_slide = scipy.stats.pearsonr(transcriptPd[slide_len:], transcriptPd_slide[0:-slide_len])[0]\n",
    "                transcriptPdSpearmanCor_slide = scipy.stats.spearmanr(transcriptPd[slide_len:], transcriptPd_slide[0:-slide_len])[0]\n",
    "\n",
    "                corPearson_slide.append(transcriptPdpearsonCor_slide)\n",
    "                corSpearman_slide.append(transcriptPdSpearmanCor_slide)\n",
    "\n",
    "                print(\"Slide test: \", f\"; slide length = {slide_len}; overlap sequence length = {len(transcriptPd[slide_len:])}\", \"; pearsonCor:\", transcriptPdpearsonCor_slide, \"; SpearmanCor: \", transcriptPdSpearmanCor_slide)\n",
    "\n",
    "            elif(slide_len<0):\n",
    "                if (args.log):\n",
    "                    transcriptPd_slide = np.exp(predictHead(seqTranscribed_slide, window_size, signalList, model))-1\n",
    "                else:\n",
    "                    transcriptPd_slide = predictHead(seqTranscribed_slide, window_size, signalList, model)\n",
    "\n",
    "                transcriptPdpearsonCor_slide = scipy.stats.pearsonr(transcriptPd[0:-slide_len], transcriptPd_slide[slide_len:])[0]\n",
    "                transcriptPdSpearmanCor_slide = scipy.stats.spearmanr(transcriptPd[0:-slide_len], transcriptPd_slide[slide_len:])[0]\n",
    "\n",
    "                corPearson_slide.append(transcriptPdpearsonCor_slide)\n",
    "                corSpearman_slide.append(transcriptPdSpearmanCor_slide)\n",
    "\n",
    "                print(\"Slide test: \", f\"; slide length = {slide_len}; overlap sequence length = {len(transcriptPd[slide_len:])}\", \"; pearsonCor:\", transcriptPdpearsonCor_slide, \"; SpearmanCor: \", transcriptPdSpearmanCor_slide)\n",
    "            \n",
    "            print(f\"Sequence length: {len(transcriptPd)}; signalList length: {len(signalList)}\", )\n",
    "            # print signal and predicted signal\n",
    "            transcriptPdpearsonCor = scipy.stats.pearsonr(transcriptPd, signalList) #np.exp()-1\n",
    "            transcriptPdSpearmanCor = scipy.stats.spearmanr(transcriptPd, signalList)\n",
    "\n",
    "            #print(f\"The M1 spearmanCor of {transcriptName} is {subseqPadPdMergeCor[0]}\")\n",
    "            corPearson.append(transcriptPdpearsonCor[0])\n",
    "            corSpearman.append(transcriptPdSpearmanCor[0])\n",
    "            # print(f\"The M1 correlation of {transcriptName}:  pearson={transcriptPdpearsonCor[0]};  spearman={transcriptPdSpearmanCor[0]}\")\n",
    "            #print(transcriptName,(np.around(np.exp(transcriptPd)-1, decimals=2)).tolist(),signalList, file=predictOP, sep=\"\\t\")\n",
    "\n",
    "            '''\n",
    "            signalPeakMark, peakCount = peakMark(signalList)\n",
    "            peakCountList.append(peakCount)\n",
    "            \n",
    "            if(peakCount == 0):\n",
    "                #roc_auc.append(0)\n",
    "                #roc_auc_prc.append(0)\n",
    "                continue\n",
    "            '''\n",
    "            ## ROC-AUC\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y_true=labelList, y_score = transcriptPd, pos_label=1)\n",
    "            aucValue = metrics.auc(fpr, tpr)\n",
    "            roc_auc.append(aucValue)\n",
    "\n",
    "            # ROCAUCPRC\n",
    "            AUCPRCValue = average_precision_score(y_true=labelList, y_score=transcriptPd, pos_label=1)\n",
    "            roc_auc_prc.append(AUCPRCValue)\n",
    "\n",
    "            if args.Signal_OP:\n",
    "                print(transcriptName,(np.around(np.exp(transcriptPd)-1, decimals=2)).tolist(),signalList, file=predictOP, sep=\"\\t\")\n",
    "                print(transcriptName, transcriptPdpearsonCor[0], transcriptPdSpearmanCor[0], aucValue, AUCPRCValue, file=predictCorOP, sep=\"\\t\")\n",
    "            # print(f\"The M1 correlation of {transcriptName}:  pearson={transcriptPdpearsonCor[0]};  spearman={transcriptPdSpearmanCor[0]}; aucValue={aucValue}; AUCPRCValue={AUCPRCValue}\")\n",
    "        elif(mode == \"midBase\"):\n",
    "\n",
    "            # 预测\n",
    "            if (args.log):\n",
    "                transcriptPd = np.exp(predictHead(seqTranscribed, window_size, signalList, model, mode=\"midBase\"))-1\n",
    "            else:\n",
    "                transcriptPd = predictHead(seqTranscribed, window_size, signalList, model, mode=\"midBase\")\n",
    "\n",
    "            # transcriptPd = transcriptPd[0]\n",
    "\n",
    "            # 通过平移sequence，测试模型泛化能力\n",
    "            if(slide_len>0):\n",
    "                if (args.log):\n",
    "                    transcriptPd_slide = np.exp(predictHead(seqTranscribed_slide, window_size, signalList, model, mode=\"midBase\"))-1\n",
    "                else:\n",
    "                    transcriptPd_slide = predictHead(seqTranscribed_slide, window_size, signalList, model, mode=\"midBase\")\n",
    "\n",
    "                transcriptPdpearsonCor_slide = scipy.stats.pearsonr(transcriptPd[slide_len:], transcriptPd_slide[0:-slide_len])[0]\n",
    "                transcriptPdSpearmanCor_slide = scipy.stats.spearmanr(transcriptPd[slide_len:], transcriptPd_slide[0:-slide_len])[0]\n",
    "\n",
    "                corPearson_slide.append(transcriptPdpearsonCor_slide)\n",
    "                corSpearman_slide.append(transcriptPdSpearmanCor_slide)\n",
    "\n",
    "                print(\"Slide test: \", f\"; slide length = {slide_len}; overlap sequence length = {len(transcriptPd[slide_len:])}\", \"; pearsonCor:\", transcriptPdpearsonCor_slide, \"; SpearmanCor: \", transcriptPdSpearmanCor_slide)\n",
    "\n",
    "            elif(slide_len<0):\n",
    "                if (args.log):\n",
    "                    transcriptPd_slide = np.exp(predictHead(seqTranscribed_slide, window_size, signalList, model))-1\n",
    "                else:\n",
    "                    transcriptPd_slide = predictHead(seqTranscribed_slide, window_size, signalList, model)\n",
    "\n",
    "                transcriptPdpearsonCor_slide = scipy.stats.pearsonr(transcriptPd[0:-slide_len], transcriptPd_slide[slide_len:])[0]\n",
    "                transcriptPdSpearmanCor_slide = scipy.stats.spearmanr(transcriptPd[0:-slide_len], transcriptPd_slide[slide_len:])[0]\n",
    "\n",
    "                corPearson_slide.append(transcriptPdpearsonCor_slide)\n",
    "                corSpearman_slide.append(transcriptPdSpearmanCor_slide)\n",
    "\n",
    "                print(\"Slide test: \", f\"; slide length = {slide_len}; overlap sequence length = {len(transcriptPd[slide_len:])}\", \"; pearsonCor:\", transcriptPdpearsonCor_slide, \"; SpearmanCor: \", transcriptPdSpearmanCor_slide)\n",
    "            \n",
    "            print(f\"Sequence length: {len(transcriptPd)}; signalList length: {len(signalList)}\", )\n",
    "            # print signal and predicted signal\n",
    "            transcriptPdpearsonCor = scipy.stats.pearsonr(transcriptPd, signalList) #np.exp()-1\n",
    "            transcriptPdSpearmanCor = scipy.stats.spearmanr(transcriptPd, signalList)\n",
    "\n",
    "            #print(f\"The M1 spearmanCor of {transcriptName} is {subseqPadPdMergeCor[0]}\")\n",
    "            corPearson.append(transcriptPdpearsonCor[0])\n",
    "            corSpearman.append(transcriptPdSpearmanCor[0])\n",
    "            # print(f\"The M1 correlation of {transcriptName}:  pearson={transcriptPdpearsonCor[0]};  spearman={transcriptPdSpearmanCor[0]}\")\n",
    "            #print(transcriptName,(np.around(np.exp(transcriptPd)-1, decimals=2)).tolist(),signalList, file=predictOP, sep=\"\\t\")\n",
    "\n",
    "            '''\n",
    "            signalPeakMark, peakCount = peakMark(signalList)\n",
    "            peakCountList.append(peakCount)\n",
    "            \n",
    "            if(peakCount == 0):\n",
    "                #roc_auc.append(0)\n",
    "                #roc_auc_prc.append(0)\n",
    "                continue\n",
    "            '''\n",
    "            ## ROC-AUC\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y_true=labelList, y_score = transcriptPd, pos_label=1)\n",
    "            aucValue = metrics.auc(fpr, tpr)\n",
    "            roc_auc.append(aucValue)\n",
    "\n",
    "            # ROCAUCPRC\n",
    "            AUCPRCValue = average_precision_score(y_true=labelList, y_score=transcriptPd, pos_label=1)\n",
    "            roc_auc_prc.append(AUCPRCValue)\n",
    "\n",
    "            if args.Signal_OP:\n",
    "                print(transcriptName,(np.around(np.exp(transcriptPd)-1, decimals=2)).tolist(),signalList, file=predictOP, sep=\"\\t\")\n",
    "                print(transcriptName, transcriptPdpearsonCor[0], transcriptPdSpearmanCor[0], aucValue, AUCPRCValue, file=predictCorOP, sep=\"\\t\")\n",
    "            # print(f\"The M1 correlation of {transcriptName}:  pearson={transcriptPdpearsonCor[0]};  spearman={transcriptPdSpearmanCor[0]}; aucValue={aucValue}; AUCPRCValue={AUCPRCValue}\")\n",
    "        if transcriptName in  TPList[0:3]: # 第一条转录本输出图\n",
    "            \n",
    "\n",
    "            def moving_average(data, smooth_window):\n",
    "                return np.convolve(data, np.ones(smooth_window)/smooth_window, mode='valid')\n",
    "\n",
    "            # 示例\n",
    "            '''\n",
    "            # data = transcriptPd # np.random.randn(100)  # 替换为你的向量数据\n",
    "            smooth_window = 3\n",
    "            smoothed_data = moving_average(transcriptPd, smooth_window)\n",
    "            print(len(transcriptPd), len(smoothed_data))\n",
    "            smoothed_data = [0]*(smooth_window//2) + smoothed_data.tolist() + [0]*(smooth_window//2)\n",
    "            '''\n",
    "\n",
    "            # 绘制原始曲线和平滑后的曲线\n",
    "            fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(15, 8), sharex=True)\n",
    "\n",
    "            ax = axes[0]\n",
    "            ax.plot(range(1, len(signalList) + 1), np.log2(np.array(signalList)), '.', markersize=2, label='signal', color = point_colors(0))\n",
    "\n",
    "            ax = axes[1]\n",
    "            ax.plot(range(1, len(transcriptPd) + 1), transcriptPd, '.', markersize=2, label='predicted signal', color = point_colors(0))\n",
    "            '''\n",
    "            ax = axes[2]\n",
    "            ax.plot(range(1, len(smoothed_data) + 1), smoothed_data, '.', markersize=2, label='smooth predicted signal', color = point_colors(0))\n",
    "            '''\n",
    "            ax = axes[2]\n",
    "            ax.plot(range(1, len(labelList) + 1), labelList, '.', markersize=2, label='labelList', color = point_colors(0))\n",
    "\n",
    "            # 隐藏右边框和上边框\n",
    "            for ax in axes:\n",
    "                ax.spines['right'].set_visible(False)\n",
    "                ax.spines['top'].set_visible(False)\n",
    "\n",
    "            plt.savefig(f'{args.output}/{transcriptName}.png')\n",
    "            plt.show()\n",
    "\n",
    "    OP = open(f\"{args.outputFolder}/EPRB_alleCLIP_modelTest.txt\", \"a+\")\n",
    "    #print(f\"Pearson correlation: {np.nanmean(corPearson)}, spearman correlation: {np.nanmean(corSpearman)}, ROCAUC: {np.nanmean(roc_auc)}, ROCAUCPRC: {np.nanmean(roc_auc_prc)}\")\n",
    "    if(slide_len != 0):\n",
    "        print(loss, protein, cell, np.nanmean(corPearson), np.nanmean(corSpearman), np.nanmean(roc_auc), np.nanmean(roc_auc_prc),np.nanmean(corPearson_slide), np.nanmean(corSpearman_slide), sep=\"\\t\", file=OP)\n",
    "    else:\n",
    "        print(loss, f\"log={log}\", protein, cell, np.nanmean(corPearson), np.nanmean(corSpearman), np.nanmean(roc_auc), np.nanmean(roc_auc_prc), sep=\"\\t\", file=OP)\n",
    "    OP.close()\n",
    "    print(loss, protein, cell, np.nanmean(corPearson), np.nanmean(corSpearman), np.nanmean(roc_auc), np.nanmean(roc_auc_prc), sep=\"\\t\")\n",
    "    # ['ENSG00000104517.13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 452)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TP_name), len(corPearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptPd = predictHead(seqTranscribed, window_size, signalList, model, mode=\"Fragment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"N\" in seqTranscribed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.transcriptBed = f\"/data/xliu/work/20231211_iDeepB/20240307_transcript_predict/TP_with_peak/TP_with_peak/QKI_HepG2_ENCSR570WLM_TP_with_peak_rmDup.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/data/xliu/work/20231211_iDeepB/20240401_trainning_gene/iDeepB_train/iDeepB_predict_transcript_head_statistics.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22435342696f32333437227d/data/xliu/work/20231211_iDeepB/20240401_trainning_gene/iDeepB_train/iDeepB_predict_transcript_head_statistics.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m TP_metric_data \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mTPList\u001b[39m\u001b[39m'\u001b[39m: TP_name, \u001b[39m'\u001b[39m\u001b[39mcorPearson\u001b[39m\u001b[39m'\u001b[39m: corPearson, \u001b[39m'\u001b[39m\u001b[39mcorSpearman\u001b[39m\u001b[39m'\u001b[39m: corSpearman, \u001b[39m'\u001b[39m\u001b[39mroc_auc\u001b[39m\u001b[39m'\u001b[39m: roc_auc, \u001b[39m'\u001b[39m\u001b[39mroc_auc_prc\u001b[39m\u001b[39m'\u001b[39m: roc_auc_prc}\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22435342696f32333437227d/data/xliu/work/20231211_iDeepB/20240401_trainning_gene/iDeepB_train/iDeepB_predict_transcript_head_statistics.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m TP_metric_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(TP_metric_data)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22435342696f32333437227d/data/xliu/work/20231211_iDeepB/20240401_trainning_gene/iDeepB_train/iDeepB_predict_transcript_head_statistics.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# 删除包含NaN的行\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22435342696f32333437227d/data/xliu/work/20231211_iDeepB/20240401_trainning_gene/iDeepB_train/iDeepB_predict_transcript_head_statistics.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m TP_metric_nan \u001b[39m=\u001b[39m TP_metric_data\u001b[39m.\u001b[39mdropna()\n",
      "File \u001b[0;32m~/softwares/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:529\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    524\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    525\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    526\u001b[0m     )\n\u001b[1;32m    528\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 529\u001b[0m     mgr \u001b[39m=\u001b[39m init_dict(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    530\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    531\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/softwares/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py:287\u001b[0m, in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    281\u001b[0m     arrays \u001b[39m=\u001b[39m [\n\u001b[1;32m    282\u001b[0m         arr \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ABCIndexClass) \u001b[39melse\u001b[39;00m arr\u001b[39m.\u001b[39m_data \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m    283\u001b[0m     ]\n\u001b[1;32m    284\u001b[0m     arrays \u001b[39m=\u001b[39m [\n\u001b[1;32m    285\u001b[0m         arr \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_datetime64tz_dtype(arr) \u001b[39melse\u001b[39;00m arr\u001b[39m.\u001b[39mcopy() \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m    286\u001b[0m     ]\n\u001b[0;32m--> 287\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, data_names, index, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[0;32m~/softwares/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py:80\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m     78\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         index \u001b[39m=\u001b[39m extract_index(arrays)\n\u001b[1;32m     81\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/softwares/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py:401\u001b[0m, in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    399\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    400\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 401\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39marrays must all be same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    403\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    404\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    405\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "TP_metric_data = {'RBPList': RBP_name, 'TPList': TP_name, 'corPearson': corPearson, 'corSpearman': corSpearman, 'roc_auc': roc_auc, 'roc_auc_prc': roc_auc_prc}\n",
    "TP_metric_data = pd.DataFrame(TP_metric_data)\n",
    "# 删除包含NaN的行\n",
    "TP_metric_nan = TP_metric_data.dropna()\n",
    "print(\"Transcript number: \", TP_metric_nan.shape)\n",
    "TP_metric_nan[['corPearson', 'corSpearman', 'roc_auc', 'roc_auc_prc']].describe()\n",
    "\n",
    "TP_metric_nan.to_csv(f\"{args.outputFolder}/iDeepB_predict_transcript_head_statistics_all.txt\", index = False, sep =\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
